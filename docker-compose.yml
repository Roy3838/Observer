
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_service # Good for user if they need to exec into it manually
    volumes:
      - ollama_data:/root/.ollama
    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    ports:
      - "11434:11434"
    restart: unless-stopped

  observer_app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
       - VITE_DISABLE_AUTH=false

    container_name: observer_app_and_proxy
    ports:
      - "8080:80"    # Web UI (Nginx)
      - "3838:3838"  # Proxy API (Python HTTPS)
    restart: unless-stopped
    depends_on:
      - ollama # Default: depends on the bundled ollama service
    environment:
      # These allow overriding the Ollama connection if the user wants to.
      # Defaults in Python code are "ollama" and "11434", which work with the bundled service.
      - OLLAMA_SERVICE_HOST=${OLLAMA_HOST_OVERRIDE:-ollama}
      - OLLAMA_SERVICE_PORT=${OLLAMA_PORT_OVERRIDE:-11434}
    # If you want to use the system's ollama in windows/macos set OLLAMA_HOST_OVERRIDE=host.docker.internal

    # Mount Docker socket only if /exec feature is active and desired
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

volumes:
  ollama_data: {}
